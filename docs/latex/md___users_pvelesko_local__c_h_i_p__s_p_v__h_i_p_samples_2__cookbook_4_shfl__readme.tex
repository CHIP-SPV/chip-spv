In this tutorial, we\textquotesingle{}ll explain how to use the warp shfl operations to improve the performance.\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md916}{}\doxysection{Introduction\+:}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md916}
Let\textquotesingle{}s talk about Warp first. The kernel code is executed in groups of fixed number of threads known as Warp. For nvidia Warp\+Size is 32 while for AMD, 32 for Polaris architecture and 64 for rest. Threads in a warp are referred to as lanes and are numbered from 0 to warp\+Size -\/1. With the help of shfl ops, we can directly exchange values of variable between threads without using any memory ops within a warp. There are four types of shfl ops\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{int   \_\_shfl      (int var,   int srcLane, int width=warpSize);}
\DoxyCodeLine{float \_\_shfl      (float var, int srcLane, int width=warpSize);}
\DoxyCodeLine{int   \_\_shfl\_up   (int var,   unsigned int delta, int width=warpSize);}
\DoxyCodeLine{float \_\_shfl\_up   (float var, unsigned int delta, int width=warpSize);}
\DoxyCodeLine{int   \_\_shfl\_down (int var,   unsigned int delta, int width=warpSize);}
\DoxyCodeLine{float \_\_shfl\_down (float var, unsigned int delta, int width=warpSize);}
\DoxyCodeLine{int   \_\_shfl\_xor  (int var,   int laneMask, int width=warpSize)}
\DoxyCodeLine{float \_\_shfl\_xor  (float var, int laneMask, int width=warpSize);}

\end{DoxyCode}
\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md917}{}\doxysection{Requirement\+:}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md917}
For hardware requirement and software installation \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/INSTALL.md}{\texttt{ Installation}}\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md918}{}\doxysection{prerequiste knowledge\+:}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md918}
Programmers familiar with CUDA, Open\+CL will be able to quickly learn and start coding with the HIP API. In case you are not, don\textquotesingle{}t worry. You choose to start with the best one. We\textquotesingle{}ll be explaining everything assuming you are completely new to gpgpu programming.\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md919}{}\doxysection{Simple Matrix Transpose}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md919}
We will be using the Simple Matrix Transpose application from the previous tutorial and modify it to learn how to use shared memory.\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md920}{}\doxysection{\+\_\+\+\_\+shfl ops}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md920}
In this tutorial, we\textquotesingle{}ll use {\ttfamily \+\_\+\+\_\+shfl()} ops. In the same sourcecode, we used for Matrix\+Transpose. We\textquotesingle{}ll add the following\+:

{\ttfamily out\mbox{[}i$\ast$width + j\mbox{]} = \+\_\+\+\_\+shfl(val,j$\ast$width + i);}

Be careful while using shfl operations, since all exchanges are possible between the threads of corresponding warp only.\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md921}{}\doxysection{How to build and run\+:}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md921}
Use the make command and execute it using ./exe Use hipcc to build the application, which is using hcc on AMD and nvcc on nvidia.\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md922}{}\doxysection{requirement for nvidia}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md922}
please make sure you have a 3.\+0 or higher compute capable device in order to use warp shfl operations and add {\ttfamily -\/gencode arch=compute=30, code=sm\+\_\+30} nvcc flag in the Makefile while using this application.\hypertarget{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md923}{}\doxysection{More Info\+:}\label{md___users_pvelesko_local__c_h_i_p__s_p_v__h_i_p_samples_2__cookbook_4_shfl__readme_autotoc_md923}

\begin{DoxyItemize}
\item \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_faq.md}{\texttt{ HIP FAQ}}
\item \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_kernel_language.md}{\texttt{ HIP Kernel Language}}
\item \href{http://rocm-developer-tools.github.io/HIP}{\texttt{ HIP Runtime API (Doxygen)}}
\item \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_porting_guide.md}{\texttt{ HIP Porting Guide}}
\item \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_terms.md}{\texttt{ HIP Terminology}} (including Rosetta Stone of GPU computing terms across CUDA/\+HIP/\+HC/\+AMP/\+OpenL)
\item \href{https://github.com/ROCm-Developer-Tools/HIPIFY/blob/master/README.md}{\texttt{ HIPIFY}}
\item \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/CONTRIBUTING.md}{\texttt{ Developer/\+CONTRIBUTING Info}}
\item \href{https://github.com/ROCm-Developer-Tools/HIP/blob/master/RELEASE.md}{\texttt{ Release Notes}} 
\end{DoxyItemize}